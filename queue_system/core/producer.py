"""
Intelligent Producer - Queue-Aware GPT Prompt Generator

=== ZWECK ===
Produziert Prompts NUR wenn Queue sie braucht
Nutzt QueueManager f√ºr Entscheidung
Nutzt FileHandler f√ºr Atomic Writes

=== FLOW ===
1. Check: Soll produziert werden? (QueueManager)
2. Ja ‚Üí W√§hle Topics aus
3. Generiere via GPT
4. Schreibe in Queue (FileHandler)
5. Log Production Event

=== KEIN BLIND PRODUCING ===
Nicht: "Generiere immer 20"
Sondern: "Frag Queue ob n√∂tig, dann wie viel"
"""
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Tuple

# Add parent to path f√ºr GPT Generator Import
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from gpt_generator.syntx_prompt_generator import generate_prompt
from gpt_generator.topics_database import get_random_topics

from .queue_manager import QueueManager
from .file_handler import FileHandler


class IntelligentProducer:
    """
    Queue-Aware Producer
    
    === DESIGN ===
    Self-Regulating - pr√ºft Queue bevor Produktion
    
    === DEPENDENCIES ===
    - QueueManager: F√ºr Decision Logic
    - FileHandler: F√ºr Atomic Writes
    - GPT Generator: F√ºr Prompt Creation
    - Topics Database: F√ºr Topic Selection
    """
    
    def __init__(self):
        """
        Initialisiert Producer mit Dependencies
        """
        self.queue_manager = QueueManager()
        self.file_handler = FileHandler()
    
    def run(self, force: bool = False) -> dict:
        """
        Haupt-Logik: Check & Produce
        
        === FLOW ===
        1. Hole Decision vom QueueManager
        2. Wenn nicht produzieren ‚Üí Skip & Log
        3. Wenn produzieren ‚Üí Generate & Write
        4. Return Stats
        
        === FORCE MODE ===
        force=True ‚Üí Ignoriert Queue-Zustand, produziert immer
        Nur f√ºr Testing/Manual Runs
        
        === ARGS ===
        force: bool - Ignoriere Queue-Check?
        
        === RETURNS ===
        dict mit:
        - should_produce: bool
        - requested_count: int
        - produced_count: int
        - skipped: bool
        - duration_seconds: float
        """
        start_time = datetime.now()
        
        # === DECISION PHASE ===
        if not force:
            should_produce, count = self.queue_manager.should_produce()
            
            if not should_produce:
                # Queue hat genug - Skip
                return {
                    "should_produce": False,
                    "requested_count": 0,
                    "produced_count": 0,
                    "skipped": True,
                    "reason": "Queue sufficient",
                    "duration_seconds": 0
                }
        else:
            # Force Mode - produziere default amount
            count = 20
            should_produce = True
        
        print(f"üîß Producer aktiviert - Generiere {count} Prompts...")
        
        # === PRODUCTION PHASE ===
        # Topics w√§hlen
        topics = get_random_topics(count)
        
        success_count = 0
        failed_count = 0
        
        # F√ºr jeden Topic: GPT ‚Üí Queue
        for i, (category, topic) in enumerate(topics, 1):
            print(f"[{i}/{count}] {category}: {topic}")
            
            # Style random w√§hlen (wie in batch_generator)
            import random
            style = random.choice(['technisch', 'kreativ', 'akademisch', 'casual'])
            
            # GPT generieren
            result = generate_prompt(
                prompt=topic,
                style=style,
                category=category,
                max_tokens=400,
                max_refusal_retries=3
            )
            
            if result['success']:
                # In Queue schreiben (atomic)
                meta_prompt = result['prompt_generated']
                
                metadata = {
                    "topic": topic,
                    "style": style,
                    "category": category,
                    "gpt_quality": result['quality_score'],
                    "gpt_cost": result['cost'],
                    "producer_run": datetime.now().isoformat()
                }
                
                try:
                    # Atomic write to queue
                    from ..config.queue_config import QUEUE_INCOMING
                    self.file_handler.atomic_write(
                        content=meta_prompt,
                        metadata=metadata,
                        target_dir=QUEUE_INCOMING
                    )
                    success_count += 1
                    print(f"   ‚úÖ In Queue geschrieben")
                except Exception as e:
                    print(f"   ‚ùå Queue Write Failed: {e}")
                    failed_count += 1
            else:
                print(f"   ‚ùå GPT Failed: {result.get('error')}")
                failed_count += 1
        
        # === STATS ===
        duration = (datetime.now() - start_time).total_seconds()
        
        stats = {
            "should_produce": should_produce,
            "requested_count": count,
            "produced_count": success_count,
            "failed_count": failed_count,
            "skipped": False,
            "duration_seconds": duration
        }
        
        print(f"\n‚úÖ Production Complete:")
        print(f"   Success: {success_count}/{count}")
        print(f"   Failed: {failed_count}")
        print(f"   Duration: {duration:.1f}s")
        
        return stats


# === MAIN BLOCK ===
if __name__ == "__main__":
    import json
    
    print("=== INTELLIGENT PRODUCER TEST ===\n")
    
    # Producer erstellen
    producer = IntelligentProducer()
    
    # Run (checkt Queue automatisch)
    stats = producer.run()
    
    print(f"\n=== STATS ===")
    print(json.dumps(stats, indent=2))
