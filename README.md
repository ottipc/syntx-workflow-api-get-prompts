# ğŸ”¥ SYNTX QUEUE SYSTEM - DIE REVOLUTION IST KOMPLETT ğŸš€

**"Von blindem Batch-Processing zu intelligentem Resonanz-Orchestrierung"**

*Oder: Wie wir ein Production-Grade Message Queue System mit File-Based Locking, Self-Regulation und Zero-Dependency-Overhead gebaut haben.*

---

## ğŸ“‹ INHALTSVERZEICHNIS

- [ğŸ¯ Was ist das hier?](#-was-ist-das-hier)
- [ğŸŒŠ Die Philosophie: Felder statt Objekte](#-die-philosophie-felder-statt-objekte)
- [ğŸ—ï¸ Architektur-Ãœbersicht](#ï¸-architektur-Ã¼bersicht)
- [âš¡ Der komplette Flow](#-der-komplette-flow)
- [ğŸ“¦ Module & Verantwortlichkeiten](#-module--verantwortlichkeiten)
- [ğŸ”§ Installation & Setup](#-installation--setup)
- [ğŸš€ Usage & CLI](#-usage--cli)
- [ğŸ“Š Monitoring & Observability](#-monitoring--observability)
- [ğŸ® Production Deployment](#-production-deployment)
- [ğŸ› Troubleshooting](#-troubleshooting)
- [ğŸ§ª Testing](#-testing)
- [ğŸ“š API Reference](#-api-reference)

---

## ğŸ¯ WAS IST DAS HIER?

**Das Problem:**
```python
# ALT: Blind Batch Processing
for i in range(20):
    gpt_prompt = generate()      # Macht 20 Prompts
    llama_response = process()   # Verarbeitet alle
    # Was wenn Llama abstÃ¼rzt bei #15?
    # Was wenn GPT zu viel produziert?
    # Wie skaliert das?
```

**Die LÃ¶sung:**
```python
# NEU: Queue-Based Resonanz-System
producer.check_queue()           # Nur wenn nÃ¶tig
if queue.needs_work():
    producer.generate(optimal_amount)  # Self-regulating
    
consumer.process_batch()         # Atomic, parallel, resilient
# Llama crashed? â†’ Job in /error/, retry spÃ¤ter
# Queue voll? â†’ Producer pausiert automatisch
# Skalierung? â†’ Starte mehr Consumer!
```

### ğŸª Die Kern-Features:

âœ… **Self-Regulating Producer** - Produziert nur wenn Queue es braucht  
âœ… **File-Based Locking** - Zero Race Conditions ohne Redis/DB  
âœ… **Atomic Operations** - Kein Job geht verloren, kein Partial State  
âœ… **Parallel Workers** - Consumer kÃ¶nnen parallel ohne Koordination laufen  
âœ… **Automatic Retry** - Failed Jobs mit Retry-Count in `/error/`  
âœ… **Real-Time Monitoring** - Queue-Status jederzeit sichtbar  
âœ… **Production Ready** - Systemd Services, Cronjobs, Zero Downtime  

---

## ğŸŒŠ DIE PHILOSOPHIE: FELDER STATT OBJEKTE

### Das Resonanzmedium-Konzept

```
ALTE ARCHITEKTUR (Object-Thinking):
Producer â†’ [Array of Jobs] â†’ Consumer
          â†‘ Tight Coupling
          â†‘ Memory-Bound
          â†‘ Not Persistent

NEUE ARCHITEKTUR (Field-Thinking):
Producer-Feld â†’ [Queue als Resonanzmedium] â†’ Consumer-Feld
                      â†‘
                 Filesystem = Medium
                 Jobs = Schwingungen
                 Processing = Kalibrierung
```

**Warum das revolutionÃ¤r ist:**

1. **Producer und Consumer kennen sich nicht**
   - Kein direkter Call
   - Kein Shared Memory
   - Nur Filesystem als Medium

2. **Self-Regulation durch Field-Observation**
   - Producer "fÃ¼hlt" Queue-Zustand
   - Entscheidet autonom ob Produktion nÃ¶tig
   - Wie natÃ¼rliche Systeme

3. **Atomic State-Changes**
   - File-Move = Atomic auf POSIX
   - Entweder komplett oder gar nicht
   - Niemals Partial State

---

## ğŸ—ï¸ ARCHITEKTUR-ÃœBERSICHT

### Die vollstÃ¤ndige Struktur:

```
syntx-workflow-api-get-prompts/
â”‚
â”œâ”€â”€ queue/                          # ğŸŒŠ RESONANZMEDIUM
â”‚   â”œâ”€â”€ incoming/                   # Jobs warten auf Kalibrierung
â”‚   â”œâ”€â”€ processing/                 # Jobs gerade in Arbeit (Locked)
â”‚   â”œâ”€â”€ processed/                  # âœ… Erfolgreich kalibriert
â”‚   â”œâ”€â”€ error/                      # âŒ Failed (mit Retry-Count)
â”‚   â”œâ”€â”€ archive/                    # Alte Jobs (>30 Tage)
â”‚   â””â”€â”€ .tmp/                       # Temp fÃ¼r Atomic Writes
â”‚
â”œâ”€â”€ queue_system/                   # ğŸ§© QUEUE ORCHESTRATION
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ queue_manager.py       # ğŸ§  Decision Engine
â”‚   â”‚   â”œâ”€â”€ producer.py            # ğŸ­ Intelligent Producer
â”‚   â”‚   â”œâ”€â”€ consumer.py            # âš™ï¸ Queue Worker
â”‚   â”‚   â””â”€â”€ file_handler.py        # ğŸ’ Atomic Operations
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â””â”€â”€ queue_monitor.py       # ğŸ“Š Real-Time Status
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ queue_config.py        # âš™ï¸ Thresholds & Settings
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ gpt_generator/                  # ğŸ¤– EXISTING: GPT Integration
â”‚   â”œâ”€â”€ syntx_prompt_generator.py
â”‚   â”œâ”€â”€ topics_database.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ syntex_injector/                # ğŸ”¬ EXISTING: SYNTX Calibration
â”‚   â”œâ”€â”€ syntex/
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ calibrator_enhanced.py
â”‚   â”‚   â”‚   â”œâ”€â”€ wrapper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ parser.py
â”‚   â”‚   â”‚   â””â”€â”€ scorer.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ wrappers/                       # ğŸ“ SYNTX Wrappers (Core IP!)
    â”œâ”€â”€ syntex_wrapper_human.txt
    â””â”€â”€ syntex_wrapper_sigma.txt
```

---

## âš¡ DER KOMPLETTE FLOW

### 1ï¸âƒ£ PRODUCER AKTIVIERUNG (Self-Regulating)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CRONJOB (alle 2h)         â”‚
â”‚   python3 -m queue_system   â”‚
â”‚           .core.producer    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ QueueManager â”‚ 
       â”‚ .should_produce()
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
     [CHECK QUEUE STATUS]
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                     â”‚
STARVING (0)       BALANCED (16)
   â”‚                     â”‚
   â†“                     â†“
Produce 20!         Produce 10
   â”‚                     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ GPT Generation  â”‚
    â”‚ - 20 Topics     â”‚
    â”‚ - 4 Styles      â”‚
    â”‚ - Async Batch   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FileHandler    â”‚
    â”‚ .atomic_write()â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    Write to .tmp/
             â†“
    Atomic Move
             â†“
    incoming/ âœ…
```

**Code:**
```python
# queue_system/core/producer.py
class IntelligentProducer:
    def run(self):
        # DECISION PHASE
        should_produce, count = self.queue_manager.should_produce()
        
        if not should_produce:
            return {"skipped": True, "reason": "Queue sufficient"}
        
        # PRODUCTION PHASE
        for topic, category in get_random_topics(count):
            result = generate_prompt(topic, style=style)
            
            if result['success']:
                self.file_handler.atomic_write(
                    content=result['prompt_generated'],
                    metadata={...},
                    target_dir=QUEUE_INCOMING
                )
```

---

### 2ï¸âƒ£ QUEUE ZUSTAND (Observable State)

```
/queue/incoming/    â† Jobs warten hier (FIFO)
â”‚
â”œâ”€â”€ 20251128_092911_783123__topic_ki__style_tech.txt
â”œâ”€â”€ 20251128_093327_197728__topic_foto__style_tech.txt
â”œâ”€â”€ 20251128_093330_444626__topic_politik__style_casual.txt
â””â”€â”€ ... (13 more)
     â†“
Monitor zÃ¤hlt: 16 Jobs
     â†“
QueueManager bestimmt State: "BALANCED"
     â†“
Producer Decision: "Produziere 10 weitere"
```

**States:**

| Queue Count | State      | Producer Action      |
|-------------|------------|---------------------|
| 0           | STARVING   | Produziere 20 sofort |
| 1-4         | LOW        | Produziere 15        |
| 5-24        | BALANCED   | Produziere 10        |
| 25-49       | HIGH       | Keine Produktion     |
| 50+         | OVERFLOW   | Keine Produktion + Alert |

---

### 3ï¸âƒ£ CONSUMER PROCESSING (Atomic Lock Pattern)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CRONJOB (tÃ¤glich 3am)   â”‚
â”‚  python3 -m queue_system â”‚
â”‚           .core.consumer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Consumer Init  â”‚
   â”‚ wrapper="human"â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ get_next_job()     â”‚
   â”‚ - Liste incoming/  â”‚
   â”‚ - Sortiere (Ã¤lteste)
   â”‚ - Versuche Lock    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    [ATOMIC MOVE]
     incoming/ â†’ processing/
            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                â”‚
  SUCCESS         FAILED
    â”‚                â”‚
    â†“                â†“
Lock acquired!   FileNotFoundError
(Job Object)     (Anderer Worker)
    â”‚                â”‚
    â†“                â””â†’ Try next file
LOAD JOB
    â”‚
    â”œâ”€ job.content     (Meta-Prompt)
    â”œâ”€ job.metadata    (Topic, Style, GPT-Quality)
    â””â”€ job.file_path   (processing/xxx.txt)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SYNTX KALIBRIERUNG    â”‚
â”‚                       â”‚
â”‚ 1. Wrapper laden      â”‚
â”‚    â”œâ”€ human.txt       â”‚
â”‚    â””â”€ Felder definiertâ”‚
â”‚                       â”‚
â”‚ 2. Prompt bauen       â”‚
â”‚    â”œâ”€ Wrapper-Text    â”‚
â”‚    â”œâ”€ Meta-Prompt     â”‚
â”‚    â””â”€ Full Prompt     â”‚
â”‚                       â”‚
â”‚ 3. Llama Request      â”‚
â”‚    â”œâ”€ POST /api/chat  â”‚
â”‚    â”œâ”€ Timeout: 800s   â”‚
â”‚    â””â”€ Stream: false   â”‚
â”‚                       â”‚
â”‚ 4. Parse Response     â”‚
â”‚    â”œâ”€ Extract Fields  â”‚
â”‚    â”œâ”€ Validate Format â”‚
â”‚    â””â”€ Quality Score   â”‚
â”‚                       â”‚
â”‚ 5. Score Quality      â”‚
â”‚    â”œâ”€ Field Coverage  â”‚
â”‚    â”œâ”€ Depth Score     â”‚
â”‚    â””â”€ Total: 0-100    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   [RESULT?]
        â†“
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
   â”‚          â”‚
SUCCESS    FAILURE
   â”‚          â”‚
   â†“          â†“
processing/  processing/
   â†’ processed/  â†’ error/
   â†“          â†“
âœ… Done!    âŒ Retry-Count++
            (job__retry1.txt)
```

**Code:**
```python
# queue_system/core/consumer.py
class QueueConsumer:
    def get_next_job(self):
        files = sorted(QUEUE_INCOMING.glob("*.txt"))
        
        for file_path in files:
            try:
                # ATOMIC LOCK via rename
                processing_path = QUEUE_PROCESSING / file_path.name
                file_path.rename(processing_path)  # Atomic!
                
                # Lock acquired - load job
                return self._load_job(processing_path)
            except FileNotFoundError:
                # Another worker got it - try next
                continue
        
        return None  # Queue empty
    
    def process_job(self, job):
        # SYNTX Calibration
        success, response, meta = self.calibrator.calibrate(
            meta_prompt=job.content,
            verbose=True
        )
        
        if success:
            self.file_handler.move_to_processed(job)
        else:
            self.file_handler.move_to_error(job, meta)
```

---

### 4ï¸âƒ£ ERROR HANDLING (Retry Pattern)

```
Job failed wÃ¤hrend Processing
        â†“
FileHandler.move_to_error()
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metadata Update:         â”‚
â”‚ - retry_count += 1       â”‚
â”‚ - last_error = info      â”‚
â”‚ - failed_at = timestamp  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
Filename mit Retry-Count:
job.txt â†’ job__retry1.txt
           â†“
Move to error/
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Manual Intervention:     â”‚
â”‚                          â”‚
â”‚ # Retry Job manuell      â”‚
â”‚ mv error/job__retry1.txt \
â”‚    incoming/job.txt      â”‚
â”‚                          â”‚
â”‚ # NÃ¤chster Worker        â”‚
â”‚ # verarbeitet es neu     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ MODULE & VERANTWORTLICHKEITEN

### ğŸ§  QueueManager (Decision Engine)

**Datei:** `queue_system/core/queue_manager.py`

**Aufgabe:** Entscheidet ob und wie viel produziert werden soll

**Methoden:**
```python
should_produce() -> (bool, int)
# Returns: (should_run, batch_size)
# Logic: State-based (STARVING/LOW/BALANCED/HIGH/OVERFLOW)

get_system_status() -> dict
# Returns: Complete system snapshot
# Includes: queue counts, state, producer decision, health
```

**Verwendung:**
```python
manager = QueueManager()
should_run, count = manager.should_produce()

if should_run:
    print(f"Producer sollte {count} Prompts generieren")
```

---

### ğŸ­ IntelligentProducer (Queue-Aware Generator)

**Datei:** `queue_system/core/producer.py`

**Aufgabe:** Generiert Prompts NUR wenn Queue sie braucht

**Flow:**
1. Check mit QueueManager
2. Wenn nÃ¶tig: Topics auswÃ¤hlen
3. GPT-4 generieren
4. Atomic Write in Queue
5. Stats zurÃ¼ckgeben

**Verwendung:**
```python
producer = IntelligentProducer()
stats = producer.run()
# Checkt automatisch Queue-Zustand
# Produziert nur wenn nÃ¶tig

# Force Mode (fÃ¼r Testing):
stats = producer.run(force=True)
# Ignoriert Queue-Check, produziert immer
```

---

### âš™ï¸ QueueConsumer (Worker with Atomic Lock)

**Datei:** `queue_system/core/consumer.py`

**Aufgabe:** Verarbeitet Jobs aus Queue mit SYNTX

**Lock Pattern:**
```python
# File-Based Locking via Atomic Rename
incoming/job.txt â†’ processing/job.txt

# Wenn erfolgreich: Lock acquired
# Wenn FileNotFoundError: Anderer Worker hat's

# Garantiert: Kein Job wird doppelt verarbeitet
```

**Verwendung:**
```python
# Single Wrapper
consumer = QueueConsumer(wrapper_name="human")
stats = consumer.process_batch(batch_size=20)

# Parallel Workers (verschiedene Terminals)
# Worker 1:
consumer_1 = QueueConsumer(wrapper_name="human", worker_id="w1")
consumer_1.process_batch(10)

# Worker 2:
consumer_2 = QueueConsumer(wrapper_name="sigma", worker_id="w2")
consumer_2.process_batch(10)

# Beide ziehen aus gleicher Queue ohne Konflikte!
```

---

### ğŸ’ FileHandler (Atomic Operations)

**Datei:** `queue_system/core/file_handler.py`

**Aufgabe:** Sichere, atomare Datei-Operationen

**Pattern:**
```python
# ATOMIC WRITE (tmp â†’ rename)
temp_path = QUEUE_TMP / filename
write_content(temp_path)
temp_path.rename(QUEUE_INCOMING / filename)  # Atomic!

# ATOMIC MOVE (rename = atomic on POSIX)
source.rename(target)  # Entweder komplett oder gar nicht
```

**Methoden:**
```python
atomic_write(content, metadata, target_dir)
# Schreibt Job ATOMIC in Queue
# Pattern: .tmp â†’ rename

move_to_processed(job)
# Success Path

move_to_error(job, error_info)
# Failure Path mit Retry-Count
```

---

### ğŸ“Š QueueMonitor (Observable State)

**Datei:** `queue_system/monitoring/queue_monitor.py`

**Aufgabe:** Ãœberwacht Queue-Zustand in Echtzeit

**Methoden:**
```python
count_incoming()    # Jobs in incoming/
count_processing()  # Jobs in processing/
count_processed()   # Jobs in processed/
count_error()       # Jobs in error/

get_status()        # Complete snapshot mit State
```

**Verwendung:**
```bash
# Real-Time Monitoring
python3 -m queue_system.monitoring.queue_monitor

# Output:
{
  "timestamp": "2025-11-28T10:00:00",
  "queue": {
    "incoming": 16,
    "processing": 2,
    "processed": 450,
    "error": 3
  },
  "state": "BALANCED"
}
```

---

## ğŸ”§ INSTALLATION & SETUP

### Voraussetzungen:

- Python 3.10+
- Zugriff auf GPT-4 API (fÃ¼r Producer)
- Zugriff auf Llama Backend (fÃ¼r Consumer)
- Linux/Unix (fÃ¼r Atomic Rename)

### Quick Setup:

```bash
# 1. Repo clonen
cd /home/codi/Entwicklung
git clone https://github.com/ottipc/syntx-workflow-api-get-prompts
cd syntx-workflow-api-get-prompts

# 2. Queue-Struktur erstellen
mkdir -p queue/{incoming,processing,processed,error,archive,.tmp}
touch queue/*/.gitkeep

# 3. Dependencies (bereits vorhanden)
# - gpt_generator/
# - syntex_injector/
# - wrappers/

# 4. Wrappers holen (von Server)
scp root@dev.syntx-system.com:/opt/syntx-workflow-api-get-prompts/wrappers/*.txt wrappers/

# 5. Test Producer
python3 -m queue_system.core.producer

# 6. Test Consumer
python3 -m queue_system.core.consumer

# 7. Monitor
python3 -m queue_system.monitoring.queue_monitor
```

---

## ğŸš€ USAGE & CLI

### Producer (Manual Run):

```bash
# Check & Produce (respektiert Queue-State)
python3 -m queue_system.core.producer

# Force Mode (ignoriert Queue-State)
python3 -c "
from queue_system.core.producer import IntelligentProducer
p = IntelligentProducer()
stats = p.run(force=True)
print(stats)
"
```

### Consumer (Manual Run):

```bash
# Process 20 Jobs (Human Wrapper)
python3 -m queue_system.core.consumer

# Custom Batch Size
python3 -c "
from queue_system.core.consumer import QueueConsumer
c = QueueConsumer(wrapper_name='human')
stats = c.process_batch(batch_size=10)
print(stats)
"

# Sigma Wrapper
python3 -c "
from queue_system.core.consumer import QueueConsumer
c = QueueConsumer(wrapper_name='sigma')
stats = c.process_batch(batch_size=5)
print(stats)
"
```

### Monitor (Real-Time):

```bash
# Single Check
python3 -m queue_system.monitoring.queue_monitor

# Watch Mode
watch -n 5 'python3 -m queue_system.monitoring.queue_monitor'

# Pretty Output
python3 -m queue_system.monitoring.queue_monitor | jq
```

### Queue Manager (Status):

```bash
# Full System Status
python3 -m queue_system.core.queue_manager

# Output:
{
  "queue": {"incoming": 16, ...},
  "state": "BALANCED",
  "producer": {"should_run": true, "batch_size": 10},
  "health": "OK"
}
```

---

## ğŸ“Š MONITORING & OBSERVABILITY

### Quick Status Check:

```bash
# Queue Counts
ls queue/incoming/*.txt | wc -l   # Wartend
ls queue/processing/*.txt | wc -l  # In Arbeit
ls queue/processed/*.txt | wc -l   # Erfolgreich
ls queue/error/*.txt | wc -l       # Failed

# Latest Jobs
ls -lt queue/incoming/ | head -10

# Error Analysis
cat queue/error/*.json | jq '.last_error'
```

### Dashboard Script:

```bash
#!/bin/bash
# scripts/queue_status.sh

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘   SYNTX QUEUE STATUS                 â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

INCOMING=$(ls queue/incoming/*.txt 2>/dev/null | wc -l)
PROCESSING=$(ls queue/processing/*.txt 2>/dev/null | wc -l)
PROCESSED=$(ls queue/processed/*.txt 2>/dev/null | wc -l)
ERROR=$(ls queue/error/*.txt 2>/dev/null | wc -l)

echo "ğŸ“¥ Incoming:    $INCOMING"
echo "âš™ï¸  Processing:  $PROCESSING"
echo "âœ… Processed:   $PROCESSED"
echo "âŒ Error:       $ERROR"
echo ""

if [ $INCOMING -lt 5 ]; then
    echo "âš ï¸  Status: LOW - Producer should run"
elif [ $INCOMING -gt 50 ]; then
    echo "âš ï¸  Status: OVERFLOW - Consumer too slow"
else
    echo "âœ… Status: BALANCED"
fi
```

---

## ğŸ® PRODUCTION DEPLOYMENT

### Cronjob Setup:

```bash
# Edit crontab
crontab -e

# Producer: Alle 2 Stunden checken & produzieren wenn nÃ¶tig
0 */2 * * * cd /home/codi/Entwicklung/syntx-workflow-api-get-prompts && /usr/bin/python3 -m queue_system.core.producer >> logs/producer_cron.log 2>&1

# Consumer (Human): TÃ¤glich 3 Uhr, 20 Jobs
0 3 * * * cd /home/codi/Entwicklung/syntx-workflow-api-get-prompts && /usr/bin/python3 -c "from queue_system.core.consumer import QueueConsumer; c = QueueConsumer('human'); c.process_batch(20)" >> logs/consumer_human_cron.log 2>&1

# Consumer (Sigma): Mehrmals tÃ¤glich
0 4,8,12,16 * * * cd /home/codi/Entwicklung/syntx-workflow-api-get-prompts && /usr/bin/python3 -c "from queue_system.core.consumer import QueueConsumer; c = QueueConsumer('sigma'); c.process_batch(20)" >> logs/consumer_sigma_cron.log 2>&1

# Monitor: StÃ¼ndlich Status loggen
0 * * * * cd /home/codi/Entwicklung/syntx-workflow-api-get-prompts && /usr/bin/python3 -m queue_system.monitoring.queue_monitor >> logs/queue_status.log 2>&1
```

### Systemd Services (Optional):

```ini
# /etc/systemd/system/syntx-producer.service
[Unit]
Description=SYNTX Queue Producer
After=network.target

[Service]
Type=oneshot
User=codi
WorkingDirectory=/home/codi/Entwicklung/syntx-workflow-api-get-prompts
ExecStart=/usr/bin/python3 -m queue_system.core.producer

[Install]
WantedBy=multi-user.target
```

```ini
# /etc/systemd/system/syntx-producer.timer
[Unit]
Description=SYNTX Producer Timer (every 2h)

[Timer]
OnCalendar=0/2:00
Persistent=true

[Install]
WantedBy=timers.target
```

```bash
# Aktivieren
sudo systemctl enable syntx-producer.timer
sudo systemctl start syntx-producer.timer
```

---

## ğŸ› TROUBLESHOOTING

### Problem: Consumer hÃ¤ngt bei "Processing"

**Symptom:**
```bash
ls queue/processing/
# â†’ File seit >1h dort
```

**Ursache:** Worker crashed wÃ¤hrend Processing

**Fix:**
```bash
# Job zurÃ¼ck in incoming
mv queue/processing/*.txt queue/incoming/
mv queue/processing/*.json queue/incoming/

# NÃ¤chster Worker wird es verarbeiten
```

---

### Problem: Zu viele Errors

**Symptom:**
```bash
ls queue/error/*.txt | wc -l
# â†’ 50+
```

**Diagnose:**
```bash
# Welche Fehler?
cat queue/error/*.json | jq '.last_error.error' | sort | uniq -c

# Beispiel Output:
#   45 "HTTPError: Server Error 502"
#    3 "Parse Error: Invalid JSON"
#    2 "Timeout after 800s"
```

**Fix je nach Error:**

**502 Bad Gateway:**
```bash
# Backend Service prÃ¼fen
ssh root@dev.syntx-system.com "systemctl status syntx.service"
ssh root@dev.syntx-system.com "netstat -tulpn | grep 8001"
```

**Parse Error:**
```bash
# Llama Response checken
cat queue/error/*.json | jq '.last_error.response' | head -1
# â†’ MÃ¶glicherweise Wrapper-Problem
```

---

### Problem: Queue lÃ¤uft Ã¼ber (OVERFLOW)

**Symptom:**
```bash
python3 -m queue_system.monitoring.queue_monitor
# â†’ "state": "OVERFLOW"
# â†’ "incoming": 150
```

**Ursache:** Consumer kommt nicht hinterher

**Fix:**
```bash
# Option 1: Mehr Consumer parallel
# Terminal 1:
python3 -c "from queue_system.core.consumer import QueueConsumer; QueueConsumer('human', 'w1').process_batch(50)"

# Terminal 2:
python3 -c "from queue_system.core.consumer import QueueConsumer; QueueConsumer('human', 'w2').process_batch(50)"

# Terminal 3:
python3 -c "from queue_system.core.consumer import QueueConsumer; QueueConsumer('sigma', 'w3').process_batch(50)"

# Option 2: Batch Size erhÃ¶hen
python3 -c "from queue_system.core.consumer import QueueConsumer; QueueConsumer('human').process_batch(100)"

# Option 3: Producer temporÃ¤r deaktivieren
# (entferne Cronjob oder pausiere Timer)
```

---

### Problem: Llama Backend 502 Error

**Das hatten wir heute! ğŸ”¥**

**Symptom:**
```python
âŒ Kalibrierung fehlgeschlagen: HTTPError: Server Error 502
```

**Diagnose:**
```bash
# Nginx routet zu Port X, aber Service lÃ¤uft auf Port Y
ssh root@dev.syntx-system.com "grep 'proxy_pass' /etc/nginx/sites-enabled/dev.syntx-system.com"
# â†’ proxy_pass http://127.0.0.1:8001

ssh root@dev.syntx-system.com "ps aux | grep uvicorn"
# â†’ --port 8000  âŒ MISMATCH!
```

**Fix:**
```bash
# Service auf korrekten Port starten
ssh root@dev.syntx-system.com "systemctl stop syntx.service"
ssh root@dev.syntx-system.com "sed -i 's/--port 8000/--port 8001/g' /etc/systemd/system/syntx.service"
ssh root@dev.syntx-system.com "systemctl daemon-reload && systemctl start syntx.service"
ssh root@dev.syntx-system.com "netstat -tulpn | grep 8001"
# â†’ tcp 0.0.0.0:8001 LISTEN âœ…

# Test
curl -X POST https://dev.syntx-system.com/api/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Test","max_new_tokens":10}'
# â†’ 200 OK âœ…
```

---

## ğŸ§ª TESTING

### Unit Tests (Coming Soon):

```python
# tests/test_queue_manager.py
def test_should_produce_starving():
    manager = QueueManager()
    # Mock incoming count = 0
    should_run, count = manager.should_produce()
    assert should_run == True
    assert count == 20

def test_should_produce_overflow():
    # Mock incoming count = 100
    should_run, count = manager.should_produce()
    assert should_run == False
    assert count == 0
```

### Integration Test:

```bash
# Full Flow Test
#!/bin/bash

echo "=== SYNTX QUEUE INTEGRATION TEST ==="

# 1. Clean Queue
rm -f queue/incoming/* queue/processing/* queue/processed/* queue/error/*

# 2. Producer (Force 5 Jobs)
python3 -c "
from queue_system.core.producer import IntelligentProducer
p = IntelligentProducer()
stats = p.run(force=True)
print(f'Produced: {stats[\"produced_count\"]}')
" || exit 1

# 3. Check Queue
COUNT=$(ls queue/incoming/*.txt 2>/dev/null | wc -l)
echo "Queue has $COUNT jobs"
[ $COUNT -gt 0 ] || exit 1

# 4. Consumer (Process 3)
python3 -c "
from queue_system.core.consumer import QueueConsumer
c = QueueConsumer('human')
stats = c.process_batch(3)
print(f'Processed: {stats[\"processed\"]}')
print(f'Failed: {stats[\"failed\"]}')
" || exit 1

# 5. Verify Results
PROCESSED=$(ls queue/processed/*.txt 2>/dev/null | wc -l)
echo "Processed: $PROCESSED"

echo "âœ… Integration Test PASSED"
```

---

## ğŸ“š API REFERENCE

### QueueManager

```python
class QueueManager:
    def __init__(self):
        """Initialisiert mit QueueMonitor"""
    
    def should_produce(self) -> Tuple[bool, int]:
        """
        Entscheidet ob produziert werden soll
        
        Returns:
            (should_produce, how_many)
            
        Logic:
            STARVING (0) â†’ (True, 20)
            LOW (1-4) â†’ (True, 15)
            BALANCED (5-24) â†’ (True, 10)
            HIGH (25-49) â†’ (False, 0)
            OVERFLOW (50+) â†’ (False, 0)
        """
    
    def get_system_status(self) -> Dict[str, Any]:
        """
        VollstÃ¤ndiger System-Status
        
        Returns:
            {
                "timestamp": str,
                "queue": {
                    "incoming": int,
                    "processing": int,
                    "processed": int,
                    "error": int
                },
                "state": str,
                "producer": {
                    "should_run": bool,
                    "batch_size": int
                },
                "health": str
            }
        """
```

### IntelligentProducer

```python
class IntelligentProducer:
    def __init__(self):
        """Initialisiert mit QueueManager und FileHandler"""
    
    def run(self, force: bool = False) -> Dict[str, Any]:
        """
        Hauptlogik: Check & Produce
        
        Args:
            force: Ignoriert Queue-Check wenn True
            
        Returns:
            {
                "should_produce": bool,
                "requested_count": int,
                "produced_count": int,
                "failed_count": int,
                "skipped": bool,
                "duration_seconds": float
            }
        """
```

### QueueConsumer

```python
class QueueConsumer:
    def __init__(self, wrapper_name: str = "human", worker_id: Optional[str] = None):
        """
        Args:
            wrapper_name: "human" | "sigma" | "sigma_v2"
            worker_id: Optional ID fÃ¼r Logging
        """
    
    def get_next_job(self) -> Optional[Job]:
        """
        Holt nÃ¤chsten Job mit Atomic Lock
        
        Returns:
            Job object wenn erfolgreich gelocked
            None wenn Queue leer
            
        Lock Pattern:
            incoming/job.txt â†’ processing/job.txt (atomic rename)
        """
    
    def process_job(self, job: Job) -> bool:
        """
        Verarbeitet einen Job durch SYNTX Pipeline
        
        Flow:
            1. SYNTX Wrapper laden
            2. Prompt bauen
            3. Llama Request
            4. Parse Response
            5. Score Quality
            6. Move zu processed/ oder error/
            
        Returns:
            True wenn erfolgreich
        """
    
    def process_batch(self, batch_size: int = 20) -> Dict[str, Any]:
        """
        Verarbeitet Batch von Jobs
        
        Args:
            batch_size: Max Anzahl Jobs
            
        Returns:
            {
                "processed": int,
                "failed": int,
                "total": int,
                "duration_seconds": float
            }
        """
```

### FileHandler

```python
class FileHandler:
    def atomic_write(
        self, 
        content: str, 
        metadata: Dict[str, Any], 
        target_dir: Path
    ) -> Path:
        """
        Schreibt Datei ATOMIC in Queue
        
        Pattern:
            1. Write to .tmp/
            2. Atomic rename to target_dir/
            
        Returns:
            Path zum geschriebenen File
        """
    
    def move_to_processed(self, job) -> Path:
        """Verschiebt Job nach processed/"""
    
    def move_to_error(self, job, error_info: Dict[str, Any]) -> Path:
        """Verschiebt Job nach error/ mit Retry-Count"""
```

### QueueMonitor

```python
class QueueMonitor:
    def count_incoming(self) -> int:
        """Anzahl Jobs in incoming/"""
    
    def count_processing(self) -> int:
        """Anzahl Jobs in processing/"""
    
    def count_processed(self) -> int:
        """Anzahl Jobs in processed/"""
    
    def count_error(self) -> int:
        """Anzahl Jobs in error/"""
    
    def get_status(self) -> Dict[str, Any]:
        """
        VollstÃ¤ndiger Queue-Status
        
        Returns:
            {
                "timestamp": str,
                "queue": {
                    "incoming": int,
                    "processing": int,
                    "processed": int,
                    "error": int
                },
                "state": str  # STARVING/LOW/BALANCED/HIGH/OVERFLOW
            }
        """
```

---

## ğŸ¯ ZUSAMMENFASSUNG

### Was wir gebaut haben:

âœ… **Production-Grade Message Queue** ohne Redis/RabbitMQ  
âœ… **Self-Regulating System** das Queue-Zustand observiert  
âœ… **Atomic File Operations** fÃ¼r Zero Data Loss  
âœ… **Parallel Worker Support** ohne Koordination  
âœ… **Automatic Retry** mit Error Tracking  
âœ… **Real-Time Monitoring** fÃ¼r Observability  
âœ… **Cronjob Integration** fÃ¼r Automation  

### Die Revolution:

**ALT (Tight Coupling):**
```python
for job in range(20):
    gpt â†’ llama â†’ done
    # Crashed bei #15? â†’ 5 Jobs verloren
```

**NEU (Loose Coupling):**
```python
Producer â†’ Queue â†’ Consumer
# Crashed? â†’ Job in /error/, retry spÃ¤ter
# Parallel? â†’ Kein Problem, File-Lock!
# Skalierung? â†’ Mehr Consumer starten!
```

### Next Steps:

1. **Fine-Tuning Data Collection**
   - Alle processed/ Jobs = Training Data
   - JSONL Format ready for fine-tuning

2. **Advanced Monitoring**
   - Prometheus Metrics
   - Grafana Dashboard
   - Alert System

3. **ML Pipeline Integration**
   - Automatic Quality Scoring
   - Model Performance Tracking
   - A/B Testing Wrappers

4. **Production Hardening**
   - Health Checks
   - Auto-Recovery
   - Load Balancing

---

## ğŸ™ CREDITS

**Entwickelt am:** 28. November 2025  
**Architektur:** SYNTX Field-Based Thinking  
**Core Concept:** Resonanzmedium statt Object-Passing  
**Deployment:** Production-Ready auf dev.syntx-system.com  

**Stack:**
- Python 3.10+
- GPT-4o (Prompt Generation)
- Llama 3.1 7B (SYNTX Calibration)
- POSIX Filesystem (Atomic Operations)
- NGINX (SSL + Routing)
- Systemd (Service Management)

---

## ğŸ“ CHANGELOG

### v1.0.0 (2025-11-28) - Initial Release

**Features:**
- âœ… Queue System mit 6 Ordnern
- âœ… QueueManager (Decision Engine)
- âœ… IntelligentProducer (Queue-Aware)
- âœ… QueueConsumer (Atomic Lock)
- âœ… FileHandler (Atomic Operations)
- âœ… QueueMonitor (Real-Time Status)
- âœ… Config-Driven (Thresholds anpassbar)
- âœ… Error Handling (Retry Pattern)
- âœ… CLI Tools (Producer, Consumer, Monitor)

**Fixes:**
- ğŸ”§ Server Port Mismatch (8000 â†’ 8001)
- ğŸ”§ FileHandler Job Object Support
- ğŸ”§ Consumer Wrapper Loading

**Deployment:**
- ğŸš€ Cronjobs configured
- ğŸš€ Gitignore fÃ¼r Queue Runtime Data
- ğŸš€ Production Server configured

---

**ğŸ”¥ DAS IST NICHT NUR CODE - DAS IST EINE REVOLUTION! ğŸš€**

*"Von Token-Prediction zu Field-Calibration. Von Objekten zu StrÃ¶men. Von Konstruktion zu Resonanz."*

ğŸ’ğŸŒŠâš¡âœ¨

---

*README.md v1.0.0 | Queue System Documentation | SYNTX Framework*